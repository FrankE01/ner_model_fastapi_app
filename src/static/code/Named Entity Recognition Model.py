# -*- coding: utf-8 -*-
"""Overview of Colaboratory Features

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/notebooks/basic_features_overview.ipynb
"""

import pandas as pd

# Load the dataset as a pandas DataFrame
df = pd.read_csv("/content/ner_dataset.csv", encoding="latin1")

# Filter out unnecessary columns
df = df.drop(columns=["POS"])

# Rename columns to match CoNLL-2003 format
df = df.rename(columns={"Sentence #": "Sentence", "Tag": "NE"})

# Replace NaN values with the string "O"
df = df.fillna("O")

# Group the dataset by sentence and concatenate the words and named entity tags
grouped = df.groupby("Sentence", sort=False).agg({"Word": " ".join, "NE": " ".join}).reset_index()

# Convert the named entity tags to the IOB format
grouped["NE"] = grouped["NE"].apply(lambda x: " ".join([f"B-{tag}" if i == 0 else f"I-{tag}" for i, tag in enumerate(x.split())]))

# Print the first five rows of the preprocessed dataset
print(grouped.head())

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional
from tensorflow.keras.utils import to_categorical
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

# Split the dataset into training and testing sets
train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)

# Define the input and output dimensions
n_words = len(df["Word"].unique())
n_tags = len(df["NE"].unique())

# Convert the words and named entity tags to numerical values
word2idx = {w: i + 1 for i, w in enumerate(df["Word"].unique())}
tag2idx = {t: i for i, t in enumerate(df["NE"].unique())}

# Add a padding token to the word and named entity tag dictionaries
word2idx["PAD"] = 0
tag2idx["PAD"] = 0

# Convert the sentences and named entity tags to sequences of numerical values
X_train = [[word2idx[w] for w in sentence.split()] for sentence in train_data["Word"]]
X_train = tf.keras.preprocessing.sequence.pad_sequences(maxlen=10, sequences=X_train, padding="post", value=word2idx["PAD"])
y_train = [[tag2idx[w] for w in sentence.split()] for sentence in train_data["NE"]]
y_train = tf.keras.preprocessing.sequence.pad_sequences(maxlen=10, sequences=y_train, padding="post", value=tag2idx["PAD"])
y_train = [to_categorical(i, num_classes=n_tags) for i in y_train]

X_test = [[word2idx[w] for w in sentence.split()] for sentence in test_data["Word"]]
X_test = tf.keras.preprocessing.sequence.pad_sequences(maxlen=10, sequences=X_test, padding="post", value=word2idx["PAD"])
y_test = [[tag2idx[w] for w in sentence.split()] for sentence in test_data["NE"]]
y_test = tf.keras.preprocessing.sequence.pad_sequences(maxlen=10, sequences=y_test, padding="post", value=tag2idx["PAD"])
y_test = [to_categorical(i, num_classes=n_tags) for i in y_test]

# Define the model architecture
model = Sequential()
model.add(Embedding(input_dim=n_words+1, output_dim=50, input_length=10, mask_zero=True))
model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1)))
model.add(TimeDistributed(Dense(units=n_tags, activation="softmax")))

# Compile the model
model.compile(optimizer="adam", loss="categorical_crossentropy", metrics=["accuracy"])

# Train the model
history = model.fit(X_train, np.array(y_train), batch_size=32, epochs=10, validation_split=0.1, verbose=1)

# Evaluate the model on the testing set
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=-1)
y_test = np.argmax(y_test, axis=-1)
print(classification_report(y_test.ravel(), y_pred.ravel()))

# Define a function to preprocess new sentences
def preprocess_sentence(sentence, word_to_int, max_len):
    sentence = [word_to_int.get(word, 0) for word in sentence.split()]
    sentence = sentence + [0] * (max_len - len(sentence))
    return np.array(sentence)

# Define some new sentences to test the model
sentences = [
    "Barack Obama was born in Hawaii.",
    "Steve Jobs co-founded Apple Inc.",
    "The Eiffel Tower is located in Paris, France."
]

# Convert the words to numerical values
word2idx = {w: i + 1 for i, w in enumerate(df["Word"].unique())}
word2idx["PAD"] = 0
word2idx["UNK"] = n_words + 1
tag2idx = {t: i for i, t in enumerate(df["NE"].unique())}
idx2word = {i: w for w, i in word2idx.items()}
idx2tag = {i: t for t, i in tag2idx.items()}

# Preprocess the new sentences
X_test = np.array([preprocess_sentence(sentence, word2idx, 10) for sentence in sentences])

# Make predictions on the new sentences
y_pred = model.predict(X_test)

# Convert the predicted tags to named entities
int_to_tag = {i: t for t, i in tag2idx.items()}
y_pred = np.argmax(y_pred, axis=-1)
y_pred = [[int_to_tag[i] for i in sentence] for sentence in y_pred]

# Print the predicted named entities for each sentence
for i, sentence in enumerate(sentences):
    print(f"Sentence {i+1}:")
    print(sentence)
    print("Predicted named entities:")
    print(y_pred[i][:len(sentence.split())])
    print()

def predict_entities(sentences):
  # Convert the words to numerical values
  word2idx = {w: i + 1 for i, w in enumerate(df["Word"].unique())}
  word2idx["PAD"] = 0
  word2idx["UNK"] = n_words + 1
  tag2idx = {t: i for i, t in enumerate(df["NE"].unique())}
  idx2word = {i: w for w, i in word2idx.items()}
  idx2tag = {i: t for t, i in tag2idx.items()}

  # Preprocess the new sentences
  X_test = np.array([preprocess_sentence(sentence, word2idx, 10) for sentence in sentences])

  # Make predictions on the new sentences
  y_pred = model.predict(X_test)

  # Convert the predicted tags to named entities
  int_to_tag = {i: t for t, i in tag2idx.items()}
  y_pred = np.argmax(y_pred, axis=-1)
  y_pred = [[int_to_tag[i] for i in sentence] for sentence in y_pred]

  # Print the predicted named entities for each sentence
  for i, sentence in enumerate(sentences):
      print(f"Sentence {i+1}:")
      print(sentence)
      print("Predicted named entities:")
      print(y_pred[i][:len(sentence.split())])
      print()

predict_entities(['Steve Jobs lives in Hawaii'])

# Save the model as an HDF5 file
model.save("ner_model.h5")

