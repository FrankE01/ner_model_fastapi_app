{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset as a pandas DataFrame\n",
        "df = pd.read_csv(\"/content/ner_dataset.csv\", encoding=\"latin1\")\n",
        "\n",
        "# Filter out unnecessary columns\n",
        "df = df.drop(columns=[\"POS\"])\n",
        "\n",
        "# Rename columns to match CoNLL-2003 format\n",
        "df = df.rename(columns={\"Sentence #\": \"Sentence\", \"Tag\": \"NE\"})\n",
        "\n",
        "# Replace NaN values with the string \"O\"\n",
        "df = df.fillna(\"O\")\n",
        "\n",
        "# Group the dataset by sentence and concatenate the words and named entity tags\n",
        "grouped = df.groupby(\"Sentence\", sort=False).agg({\"Word\": \" \".join, \"NE\": \" \".join}).reset_index()\n",
        "\n",
        "# Convert the named entity tags to the IOB format\n",
        "grouped[\"NE\"] = grouped[\"NE\"].apply(lambda x: \" \".join([f\"B-{tag}\" if i == 0 else f\"I-{tag}\" for i, tag in enumerate(x.split())]))\n",
        "\n",
        "# Print the first five rows of the preprocessed dataset\n",
        "print(grouped.head())"
      ],
      "metadata": {
        "id": "eEFLzaDnVZnW",
        "outputId": "d7e609e3-9616-4c70-bd2b-db54486acca9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Sentence                                               Word  \\\n",
            "0  Sentence: 1                                          Thousands   \n",
            "1            O  of demonstrators have marched through London t...   \n",
            "2  Sentence: 2                                           Families   \n",
            "3  Sentence: 3                                               They   \n",
            "4  Sentence: 4                                             Police   \n",
            "\n",
            "                                                  NE  \n",
            "0                                                B-O  \n",
            "1  B-O I-O I-O I-O I-O I-B-geo I-O I-O I-O I-O I-...  \n",
            "2                                                B-O  \n",
            "3                                                B-O  \n",
            "4                                                B-O  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "metadata": {
        "id": "RBW2p4QuXWvD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "OwDku1JsXWrO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input and output dimensions\n",
        "n_words = len(df[\"Word\"].unique())\n",
        "n_tags = len(df[\"NE\"].unique())"
      ],
      "metadata": {
        "id": "eXKODWsoXduu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the words and named entity tags to numerical values\n",
        "word2idx = {w: i + 1 for i, w in enumerate(df[\"Word\"].unique())}\n",
        "tag2idx = {t: i for i, t in enumerate(df[\"NE\"].unique())}\n"
      ],
      "metadata": {
        "id": "wTu94EMPXho0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a padding token to the word and named entity tag dictionaries\n",
        "word2idx[\"PAD\"] = 0\n",
        "tag2idx[\"PAD\"] = 0"
      ],
      "metadata": {
        "id": "-Us6u9mQXlYM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the sentences and named entity tags to sequences of numerical values\n",
        "X_train = [[word2idx[w] for w in sentence.split()] for sentence in train_data[\"Word\"]]\n",
        "X_train = tf.keras.preprocessing.sequence.pad_sequences(maxlen=10, sequences=X_train, padding=\"post\", value=word2idx[\"PAD\"])\n",
        "y_train = [[tag2idx[w] for w in sentence.split()] for sentence in train_data[\"NE\"]]\n",
        "y_train = tf.keras.preprocessing.sequence.pad_sequences(maxlen=10, sequences=y_train, padding=\"post\", value=tag2idx[\"PAD\"])\n",
        "y_train = [to_categorical(i, num_classes=n_tags) for i in y_train]\n",
        "\n",
        "X_test = [[word2idx[w] for w in sentence.split()] for sentence in test_data[\"Word\"]]\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(maxlen=10, sequences=X_test, padding=\"post\", value=word2idx[\"PAD\"])\n",
        "y_test = [[tag2idx[w] for w in sentence.split()] for sentence in test_data[\"NE\"]]\n",
        "y_test = tf.keras.preprocessing.sequence.pad_sequences(maxlen=10, sequences=y_test, padding=\"post\", value=tag2idx[\"PAD\"])\n",
        "y_test = [to_categorical(i, num_classes=n_tags) for i in y_test]"
      ],
      "metadata": {
        "id": "vTl0zAW1X2xc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=n_words+1, output_dim=50, input_length=10, mask_zero=True))\n",
        "model.add(Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1)))\n",
        "model.add(TimeDistributed(Dense(units=n_tags, activation=\"softmax\")))\n"
      ],
      "metadata": {
        "id": "0vp0imq7YccE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "jR_Xd7uyYdUl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(X_train, np.array(y_train), batch_size=32, epochs=10, validation_split=0.1, verbose=1)\n"
      ],
      "metadata": {
        "id": "hDNNuIxcYhAo",
        "outputId": "a185fee7-63be-432f-9e95-6de52cbc7d2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "23593/23593 [==============================] - 1710s 72ms/step - loss: 0.2346 - accuracy: 0.9364 - val_loss: 0.1898 - val_accuracy: 0.9452\n",
            "Epoch 2/10\n",
            "23593/23593 [==============================] - 1648s 70ms/step - loss: 0.1585 - accuracy: 0.9540 - val_loss: 0.1865 - val_accuracy: 0.9466\n",
            "Epoch 3/10\n",
            "23593/23593 [==============================] - 1616s 69ms/step - loss: 0.1479 - accuracy: 0.9561 - val_loss: 0.1881 - val_accuracy: 0.9460\n",
            "Epoch 4/10\n",
            "23593/23593 [==============================] - 1614s 68ms/step - loss: 0.1440 - accuracy: 0.9567 - val_loss: 0.1921 - val_accuracy: 0.9431\n",
            "Epoch 5/10\n",
            "23593/23593 [==============================] - 1662s 70ms/step - loss: 0.1421 - accuracy: 0.9569 - val_loss: 0.1951 - val_accuracy: 0.9386\n",
            "Epoch 6/10\n",
            "23593/23593 [==============================] - 1646s 70ms/step - loss: 0.1410 - accuracy: 0.9569 - val_loss: 0.1987 - val_accuracy: 0.9372\n",
            "Epoch 7/10\n",
            "23593/23593 [==============================] - 1660s 70ms/step - loss: 0.1402 - accuracy: 0.9569 - val_loss: 0.2033 - val_accuracy: 0.9370\n",
            "Epoch 8/10\n",
            "23593/23593 [==============================] - 1673s 71ms/step - loss: 0.1396 - accuracy: 0.9569 - val_loss: 0.2040 - val_accuracy: 0.9372\n",
            "Epoch 9/10\n",
            "23593/23593 [==============================] - 1656s 70ms/step - loss: 0.1393 - accuracy: 0.9570 - val_loss: 0.2057 - val_accuracy: 0.9369\n",
            "Epoch 10/10\n",
            "23593/23593 [==============================] - 1675s 71ms/step - loss: 0.1388 - accuracy: 0.9570 - val_loss: 0.2050 - val_accuracy: 0.9372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_test = np.argmax(y_test, axis=-1)\n",
        "print(classification_report(y_test.ravel(), y_pred.ravel()))"
      ],
      "metadata": {
        "id": "U6NxLNaBYkOU",
        "outputId": "830619ce-60d9-4210-b619-4863781d9363",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6554/6554 [==============================] - 68s 10ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00   2065171\n",
            "           1       0.78      0.85      0.81      7497\n",
            "           2       0.94      0.92      0.93      3186\n",
            "           3       0.62      0.67      0.64      3354\n",
            "           4       0.70      0.59      0.64      1430\n",
            "           5       0.37      0.55      0.45      4076\n",
            "           6       0.66      0.51      0.58      3201\n",
            "           7       0.87      0.76      0.81      4037\n",
            "           8       0.30      0.04      0.07        77\n",
            "           9       0.08      0.01      0.02        75\n",
            "          10       0.72      0.69      0.71      3468\n",
            "          11       0.64      0.64      0.64        42\n",
            "          12       0.59      0.16      0.25      1374\n",
            "          13       0.38      0.39      0.38        38\n",
            "          14       0.34      0.32      0.33        60\n",
            "          15       0.33      0.02      0.04        53\n",
            "          16       0.00      0.00      0.00        11\n",
            "\n",
            "    accuracy                           0.99   2097150\n",
            "   macro avg       0.55      0.48      0.49   2097150\n",
            "weighted avg       0.99      0.99      0.99   2097150\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to preprocess new sentences\n",
        "def preprocess_sentence(sentence, word_to_int, max_len):\n",
        "    sentence = [word_to_int.get(word, 0) for word in sentence.split()]\n",
        "    sentence = sentence + [0] * (max_len - len(sentence))\n",
        "    return np.array(sentence)\n",
        "\n",
        "# Define some new sentences to test the model\n",
        "sentences = [\n",
        "    \"Barack Obama was born in Hawaii.\",\n",
        "    \"Steve Jobs co-founded Apple Inc.\",\n",
        "    \"The Eiffel Tower is located in Paris, France.\"\n",
        "]\n",
        "\n",
        "# Convert the words to numerical values\n",
        "word2idx = {w: i + 1 for i, w in enumerate(df[\"Word\"].unique())}\n",
        "word2idx[\"PAD\"] = 0\n",
        "word2idx[\"UNK\"] = n_words + 1\n",
        "tag2idx = {t: i for i, t in enumerate(df[\"NE\"].unique())}\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "idx2tag = {i: t for t, i in tag2idx.items()}\n",
        "\n",
        "# Preprocess the new sentences\n",
        "X_test = np.array([preprocess_sentence(sentence, word2idx, 10) for sentence in sentences])\n",
        "\n",
        "# Make predictions on the new sentences\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert the predicted tags to named entities\n",
        "int_to_tag = {i: t for t, i in tag2idx.items()}\n",
        "y_pred = np.argmax(y_pred, axis=-1)\n",
        "y_pred = [[int_to_tag[i] for i in sentence] for sentence in y_pred]\n",
        "\n",
        "# Print the predicted named entities for each sentence\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"Sentence {i+1}:\")\n",
        "    print(sentence)\n",
        "    print(\"Predicted named entities:\")\n",
        "    print(y_pred[i][:len(sentence.split())])\n",
        "    print()\n",
        "\n"
      ],
      "metadata": {
        "id": "iy6Jtkn3irwM",
        "outputId": "3c83cb88-7bad-4a3b-f4ba-d2695687448c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 845ms/step\n",
            "Sentence 1:\n",
            "Barack Obama was born in Hawaii.\n",
            "Predicted named entities:\n",
            "['I-per', 'I-per', 'O', 'O', 'O', 'O']\n",
            "\n",
            "Sentence 2:\n",
            "Steve Jobs co-founded Apple Inc.\n",
            "Predicted named entities:\n",
            "['B-per', 'O', 'O', 'B-org', 'I-org']\n",
            "\n",
            "Sentence 3:\n",
            "The Eiffel Tower is located in Paris, France.\n",
            "Predicted named entities:\n",
            "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_entities(sentences):\n",
        "  # Convert the words to numerical values\n",
        "  word2idx = {w: i + 1 for i, w in enumerate(df[\"Word\"].unique())}\n",
        "  word2idx[\"PAD\"] = 0\n",
        "  word2idx[\"UNK\"] = n_words + 1\n",
        "  tag2idx = {t: i for i, t in enumerate(df[\"NE\"].unique())}\n",
        "  idx2word = {i: w for w, i in word2idx.items()}\n",
        "  idx2tag = {i: t for t, i in tag2idx.items()}\n",
        "\n",
        "  # Preprocess the new sentences\n",
        "  X_test = np.array([preprocess_sentence(sentence, word2idx, 10) for sentence in sentences])\n",
        "\n",
        "  # Make predictions on the new sentences\n",
        "  y_pred = model.predict(X_test)\n",
        "\n",
        "  # Convert the predicted tags to named entities\n",
        "  int_to_tag = {i: t for t, i in tag2idx.items()}\n",
        "  y_pred = np.argmax(y_pred, axis=-1)\n",
        "  y_pred = [[int_to_tag[i] for i in sentence] for sentence in y_pred]\n",
        "\n",
        "  # Print the predicted named entities for each sentence\n",
        "  for i, sentence in enumerate(sentences):\n",
        "      print(f\"Sentence {i+1}:\")\n",
        "      print(sentence)\n",
        "      print(\"Predicted named entities:\")\n",
        "      print(y_pred[i][:len(sentence.split())])\n",
        "      print()"
      ],
      "metadata": {
        "id": "PnbfPrSloI2d"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_entities(['Steve Jobs lives in Hawaii'])"
      ],
      "metadata": {
        "id": "QfWnlqvyrn6s",
        "outputId": "e216b257-ab1b-46db-cef8-871942909972",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 58ms/step\n",
            "Sentence 1:\n",
            "Steve Jobs lives in Hawaii\n",
            "Predicted named entities:\n",
            "['B-per', 'O', 'O', 'O', 'B-geo']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model as an HDF5 file\n",
        "model.save(\"ner_model.h5\")"
      ],
      "metadata": {
        "id": "JsJ9NJWO1FC8"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RDVGaACO1MGs"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}